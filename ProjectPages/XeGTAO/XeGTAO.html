<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <script
      src="https://kit.fontawesome.com/59ba4b56ec.js"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://code.jquery.com/jquery-3.6.0.min.js"
      integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4="
      crossorigin="anonymous"
    ></script>
    <script src="http://code.jquery.com/jquery-migrate-3.0.0.min.js"></script>
    <script src="http://code.jquery.com/mobile/1.4.5/jquery.mobile-1.4.5.min.js"></script>

    <script src="/assets/js/script-min.js"></script>
    <!--IGHF Loader-->
    <!--[if le IE 9]>
      <script
        type="text/javascript"
        src="https://www.intel.com/ighf/50recode.2/js/headerchooser.js"
        async
        defer
      ></script>
    <![endif]-->
    <!--[if gt IE 9]><!-->
    <script
      type="text/javascript"
      src="https://www.intel.com/ighf/50recode.2/js/headerchooser.js"
      async
    ></script>
    <!--<![endif]-->
    <script>
      INTELNAV = window.INTELNAV || {};
      INTELNAV.renderSettings = {
        version: "2.0 - 03/12/2017 08:00:00",
        textDirection: "--",
        culture: "--",
        OutputId: "--",
      };
    </script>
    <!--/IGHF Loader-->
    <link
      rel="stylesheet"
      href="https://unpkg.com/flickity@2/dist/flickity.min.css"
    />
    <script src="https://unpkg.com/flickity@2/dist/flickity.pkgd.min.js"></script>
    <link
      rel="stylesheet"
      href="https://gametechdev.github.io/assets/css/main.css"
    />

    <!-- Begin Jekyll SEO tag v2.7.1 -->
    <title>XeGTAO | GameTechDev</title>
    <meta name="generator" content="Jekyll v4.2.0" />
    <meta property="og:title" content="XeGTAO" />
    <meta property="og:locale" content="en_US" />
    <meta
      name="description"
      content="Helping developers create the best gaming experiences on Intel graphics processors."
    />
    <meta
      property="og:description"
      content="Helping developers create the best gaming experiences on Intel graphics processors."
    />
    <link
      rel="canonical"
      href="https://gametechdev.github.io/ProjectPages/XeGTAO/XeGTAO.html"
    />
    <meta
      property="og:url"
      content="https://gametechdev.github.io/ProjectPages/XeGTAO/XeGTAO.html"
    />
    <meta property="og:site_name" content="GameTechDev" />
    <meta name="twitter:card" content="summary" />
    <meta property="twitter:title" content="XeGTAO" />
    <meta name="twitter:site" content="@intel" />
    <script type="application/ld+json">
      {
        "description": "Helping developers create the best gaming experiences on Intel graphics processors.",
        "headline": "XeGTAO",
        "url": "https://gametechdev.github.io/ProjectPages/XeGTAO/XeGTAO.html",
        "@type": "WebPage",
        "@context": "https://schema.org"
      }
    </script>
    <!-- End Jekyll SEO tag -->
  </head>

  <body>
    <!--IGHF Header-->
    <!--GAATversion='50recode.2' date='09/11/2017 08:00:00' Version='2.0':CharacterEncoding:utf8-->
    <div id="recode50header" class="no-animate"></div>
    <script type="text/javascript">
      /*<![CDATA[*/
      INTELNAV = window.INTELNAV || {};
      INTELNAV.renderSettings = {
        version: "2.0 - 03/12/2017 08:00:00",
        textDirection: "ltr",
        culture: "en_US",
        OutputId: "default",
      };
      /*]]>*/
    </script>
    <noscript>
      <link
        rel="stylesheet"
        href="https://www.intel.com/ighf/50recode.2/css/ltr_nojsheader.css"
        type="text/css"
      />
      <div id="smallfootprint-header">
        <a
          href="https://www.intel.com/content/www/us/en/homepage.html"
          class="gaat40-logo"
          title="Logo - Intel"
        ></a>
        <form
          method="get"
          id="user-bar-searchbox-form"
          name="user-bar-searchbox-form"
          action="https://www.intel.com/content/www/us/en/search.html"
        >
          <fieldset>
            <legend></legend>
            <label for="input-search">Search</label
            ><input type="text" id="input-search" name="keyword" /><input
              type="submit"
              id="input-submit"
              name="input-submit"
              value="Search"
            />
          </fieldset>
        </form>
      </div>
    </noscript>
    <!--/IGHF Header-->
    <header>
      <div class="header-left">
        <a href="https://gametechdev.github.io">
          <div class="logo">
            <img
              src="https://gametechdev.github.io/assets/img/logo.svg"
              alt="IntelÂ®"
            />
          </div>
        </a>

        <h1><a href="https://gametechdev.github.io">GameTechDev</a></h1>
      </div>

      <div id="mobile-menu-toggle">
        <i id="menu-i" class="far fa-fw fa-bars" onClick="menuToggler()"></i>
        <i
          id="close-i"
          class="far fa-fw fa-times"
          onClick="menuToggler()"
          style="display: none"
        ></i>
      </div>

      <nav id="header-nav">
        <ul>
          <li class="">
            <a href="https://gametechdev.github.io/projects">Projects</a>
          </li>

          <li class="">
            <a href="https://gametechdev.github.io/documentation"
              >Documentation</a
            >
          </li>

          <li class="">
            <a href="https://gametechdev.github.io/tools-and-libraries"
              >Tools and Libraries</a
            >
          </li>
        </ul>
      </nav>
    </header>
    <main>
      <h1>XeGTAO</h1>
      <h2 id="introduction">Introduction</h2>

      <p>
        XeGTAO is an open source, MIT licensed, DirectX/HLSL implementation of
        the
        <em
          >Practical Realtime Strategies for Accurate Indirect Occlusion,
          GTAO</em
        >
        <a
          href="https://www.activision.com/cdn/research/Practical_Real_Time_Strategies_for_Accurate_Indirect_Occlusion_NEW%20VERSION_COLOR.pdf"
          >[Jimenez et al., 2016]</a
        >, a screen space effect suitable for use on a wide range of modern PC
        integrated and discrete GPUs. The main benefit of GTAO over other screen
        space algorithms is that it uses a radiometrically-correct ambient
        occlusion equation, providing more physically correct AO term.
      </p>

      <p>
        We have implemented and tested the core algorithm that computes and
        spatially filters the ambient occlusion integral. Implementing the
        Directional GTAO component (bent normals) is the next planned step.
      </p>

      <p>
        Our implementation relies on an integrated spatial denoising filter and
        will leverage TAA for temporal accumulation when available. When used in
        conjunction with TAA it is both faster, provides higher detail effect on
        fine geometry features and is more radiometrically correct than other
        common public SSAO implementations such as closed the source
        <a
          href="https://www.nvidia.com/en-gb/geforce/technologies/hbao-plus/technology/"
          >[HBAO+]</a
        >, and open source
        <a
          href="https://software.intel.com/content/www/us/en/develop/articles/adaptive-screen-space-ambient-occlusion.html"
          >[ASSAO]</a
        >.
      </p>

      <p>
        High quality preset, computed at full resolution, costs roughly 1.4ms at
        3840x2160 on RTX 3070, 0.56ms at 1920x1080 on RTX 2060 and 2.39ms at
        1920x1080 on 11th Gen Intel(R) Core(TM) i7-1195G7 integrated graphics.
        Faster but lower quality preset is also available.
      </p>

      <p>
        This sample project (<a
          href="https://github.com/GameTechDev/XeGTAO/tree/master/VisualStudio/Vanilla.sln"
          >Vanilla.sln</a
        >) was tested with Visual Studio 2019 16.10.3, DirectX 12 GPU, Windows
        version 10.0.19041.
      </p>

      <p>
        XeGTAO OFF/ON comparison in Amazon Lumberyard Bistro; click on image to
        embiggen:
      </p>
      <figure class="comparison-container">
        <img src="images/exteriorXeGTAO.png" alt="" />
        <span class="image-label" data-type="original">XeGTAO</span>

        <div
          class="resize-image"
          style="background: url('images/exteriorASSAO.png') no-repeat left top"
        >
          <span class="image-label" data-type="modified">ASSAO</span>
        </div>

        <span class="handle"><i class="fas fa-arrows-alt-h"></i></span>
      </figure>

      <p>
        <a href="exterior_compare.html"
          ><img src="images/thumb-exterior-off.png" alt="thumb1"
        /></a>
        <a href="exterior_compare.html"
          ><img src="images/thumb-exterior-GTAO.png" alt="thumb2"
        /></a>
        <a href="interior_compare.html"
          ><img src="images/thumb-interior-off.png" alt="thumb3"
        /></a>
        <a href="interior_compare.html"
          ><img src="images/thumb-interior-GTAO.png" alt="thumb4"
        /></a>
      </p>

      <p>
        AO term only, left: ASSAO Medium (~0.72ms), right: XeGTAO High
        (~0.56ms), as measured on RTX 2060 at 1920x1080:
      </p>

      <h2 id="implementation-and-integration-overview">
        Implementation and integration overview
      </h2>

      <p>
        We focus on simplicity and ease of integration with all relevant code
        provided in a 2-file, header only-like format:
      </p>
      <ul>
        <li>
          <a
            href="https://github.com/GameTechDev/XeGTAO/tree/master/Source/Rendering/Shaders/XeGTAO.h"
            ><strong>XeGTAO.h</strong></a
          >
          provides the glue between the user codebase and the effect; this is
          where macro definitions, settings, constant buffer updates and
          optional ImGui debug settings are handled.
        </li>
        <li>
          <a
            href="https://github.com/GameTechDev/XeGTAO/tree/master/Source/Rendering/Shaders/XeGTAO.hlsli"
            ><strong>XeGTAO.hlsli</strong></a
          >
          provides the core shader code for the effect.
        </li>
      </ul>

      <p>
        These two files contain the minimum required to integrate the effect,
        with the amount of work depending on the specific platform and engine
        details. In an ideal case, the user codebase can include these two files
        above with little or no modifications, and provide additional resources
        including working textures, a constant buffer, compute shaders, as well
        as codebase-specific shader code used to load screen space normals and
        etc, as shown in the usage example (see
        <a
          href="https://github.com/GameTechDev/XeGTAO/tree/master/Source/Rendering/Effects/vaGTAO.h"
          >vaGTAO.h</a
        >,
        <a
          href="https://github.com/GameTechDev/XeGTAO/tree/master/Source/Rendering/Effects/vaGTAO.cpp"
          >vaGTAO.cpp</a
        >,
        <a
          href="https://github.com/GameTechDev/XeGTAO/tree/master/Source/Rendering/Effects/vaGTAO.hlsl"
          >vaGTAO.hlsl</a
        >).
      </p>

      <p>
        The effect is usually computed just after the depth data becomes
        available (after the depth pre-pass or a g-buffer draw). It takes depth
        buffer and (optional) screen space normals as inputs, produces a single
        channel AO buffer as the output and consists of three separate compute
        shader passes:
      </p>
      <ul>
        <li>
          <a
            href="https://github.com/GameTechDev/XeGTAO/tree/master/Source/Rendering/Shaders/XeGTAO.hlsli#L486"
            ><strong>PrefilterDepths pass</strong></a
          >: inputs depth buffer; performs input depth conversion to viewspace
          and generation of depth MIP chain; outputs intermediary viewspace
          depth buffer with a MIP chain
        </li>
        <li>
          <a
            href="https://github.com/GameTechDev/XeGTAO/tree/master/Source/Rendering/Shaders/XeGTAO.hlsli#L167"
            ><strong>MainPass</strong></a
          >: inputs intermediary depth buffer and (optional) screen space
          normals; performs the core GTAO algorithm; outputs unfiltered AO term
          and intermediary edge information by the denoiser
        </li>
        <li>
          <a
            href="https://github.com/GameTechDev/XeGTAO/tree/master/Source/Rendering/Shaders/XeGTAO.hlsli#L596"
            ><strong>Denoise</strong></a
          >: inputs unfiltered AO term and intermediary edge information;
          performs the spatial denoise filter; outputs the final AO term
        </li>
      </ul>

      <h2 id="implementation-details">Implementation details</h2>

      <p>
        Following is a list of implementation details and differences from the
        original GTAO paper
        <a
          href="https://www.activision.com/cdn/research/Practical_Real_Time_Strategies_for_Accurate_Indirect_Occlusion_NEW%20VERSION_COLOR.pdf"
          >[Jimenez et al., 2016]</a
        >:
      </p>

      <h3 id="automatic-heuristic-tuning-based-on-ray-traced-reference">
        Automatic heuristic tuning based on ray-traced reference
      </h3>

      <p>
        In order to best reproduce the work from the original paper and tune the
        heuristics, we built a simple ray tracer within the development codebase
        that can render AO ground truth. The ray tracer uses cosine-weighted
        hemisphere Monte Carlo sampling to approximate Lambertian reflectance
        for a hemisphere defined by a given point and geometry normal, using
        near-field bounding long visibility rays.
      </p>

      <p>
        <img
          src="images/reference-raytracer.png"
          alt="Reference Raytracer"
          width="100%"
        />
        <em
          >left: Reference diffuse-only raytracer, 512spp; right: XeGTAO High
          preset</em
        >
      </p>

      <p>
        Using the ray-traced output as a ground truth, we then tune the XeGTAO
        heuristics for a best match across several scenes and locations and
        different near-field bound radii settings. We rely in big part on an
        automatic system informally called auto-tune, where selected settings
        (such as thickness heuristic, radius multiplier, falloff range, etc.)
        can be automatically tuned together. Given min/max ranges for each
        setting, the auto-tune will run through all permutations across
        pre-defined scene locations, searching for the lowest overall average
        MSE between the XeGTAO and raytraced ground outputs. For practical
        reasons we employ a multi-pass search based on narrowing the setting
        ranges.
      </p>

      <h3 id="denoising">Denoising</h3>
      <p>
        Original GTAO implementation is described as using spatio-temporal
        sampling with temporal reprojection; our current approach uses only a
        5x5 depth-aware spatial denoising filter and relies on TAA for the
        temporal component, when available. This is a compromise that allows for
        the effect to still be used by codebases that do not employ TAA. When
        TAA is available, we indirectly leverage it by enabling temporal noise.
        The downside is that we must keep temporal variance low enough to avoid
        having TAA mischaracterizing this noise as features, which limits the
        amount of temporal supersampling that we can leverage. Depending on user
        feedback and future experimentation we are likely to go with the
        combined spatio-temporal approach in the future.
      </p>

      <p>
        <img src="images/denoising.png" alt="denoising" width="100%" />
        <em
          >left: raw 3 slices 6 samples per pixel (18spp) XeGTAO output; middle:
          +5x5 spatial denoiser; right: +TAA and temporal noise</em
        >
      </p>

      <h3 id="resolution-and-sampling">Resolution and sampling</h3>

      <p>
        Original GTAO implementation runs at half-resolution with one slice per
        pixel, 12 samples per hemisphere slice (6 per side; the
        <em>hemisphere slice</em> term is defined in the original paper). We
        default to running at full resolution, 3 slices per pixel with 6 samples
        (3 per side) each for a total of 18spp, and also have a lower quality
        preset with 2 slices per pixel and 4 samples (2 per side) for a total of
        8spp. The reasoning behind reduction in samples per slice is described
        in âThickness Heuristicâ section. This balance is likely to change if we
        move to a combined spatio-temporal approach in the future.
      </p>

      <h3 id="sample-distribution">Sample distribution</h3>
      <p>
        In order to better capture thin crevices and similar small features, we
        use x = pow( x, 2 ) distribution for samples along the slice direction,
        where x is a normalized screen space distance from the evaluated pixelâs
        center to the maximum distance (representing the worldspace âEffect
        radiusâ). This is another setting where we used auto-tune to find the
        most optimal value, which was around 2.1. We decided to round it down to
        2 for simplicity and performance reasons.
      </p>

      <p>
        <img
          src="images/sample-distribution-power.png"
          alt="denoising"
          width="100%"
        />
        <em
          >different sample power distribution settings; left: setting of 1.0;
          right: setting of 2.0, clumping more samples around the center</em
        >
      </p>

      <h3 id="near-field-bounding">Near-field bounding</h3>
      <p>
        Like the
        <a
          href="https://www.activision.com/cdn/research/Practical_Real_Time_Strategies_for_Accurate_Indirect_Occlusion_NEW%20VERSION_COLOR.pdf"
          >[Jimenez et al., 2016]</a
        >, we attenuate the effect from distant samples based on near-field
        occlusion radius setting (âEffect radiusâ) over a a range (âFalloff
        rangeâ). This provides stable and predictable results and is easier to
        use in conjunction with longer-range, lower-frequency GI. Unlike the
        original, we do not linearly interpolate the sample horizon angle cosine
        towards -1 but towards the hemisphere horizon, computed as
        cos(normal_angle+PI/2) in one direction and cos(normal_angle-PI/2) in
        the other.
      </p>

      <p>
        <img src="images/falloff-types.png" alt="falloff types" width="100%" />
        <em
          >out of bounds sample interpolation, left: towards -1; middle: ray
          traced ground truth; right: ours, towards sample horizon, resulting in
          less detail loss, noticeable around the window and curtain areas</em
        >
      </p>

      <p>
        This makes the attenuation function independent from the projected
        normal vector, avoid haloing or loss of detail under certain view
        angles, providing results that are on average closer to the ground
        truth.
      </p>

      <h3 id="thin-occluder-conundrum">Thin occluder conundrum</h3>
      <p>
        The main difficulty of approximating AO from the depth buffer is that
        the depth buffer is effectively a viewspace heightmap and does not
        correctly represent the actual scene geometry. This leads to visual
        artifacts such as thin features at depth discontinuities casting too
        much occlusion (please see âHeight-field assumption considerationsâ from
        the
        <a
          href="https://www.activision.com/cdn/research/Practical_Real_Time_Strategies_for_Accurate_Indirect_Occlusion_NEW%20VERSION_COLOR.pdf"
          >[Jimenez et al., 2016]</a
        >) or haloing effects. The larger the near-field bounding radius
        setting, the worse the mismatch usually is. Conversely, with a radius
        that is small in proportion to geometry features, and using various
        heuristics to minimize the side-effects, a reasonably good approximation
        can be achieved. There are other solutions to improving the quality of
        the source geometry representation (such as Multi-view AO,
        <a href="https://dl.acm.org/doi/10.1145/2448196.2448214"
          >[Vardis et al. 2013]</a
        >, or the more recent Stochastic-Depth Ambient Occlusion
        <a href="https://dl.acm.org/doi/10.1145/3451268"
          >[Vermeer et al, 2021]</a
        >
        ) which we did not consider due to complexity as they require changes to
        the rendering pipeline, but which could certainly be adopted for use
        with XeGTAO.
      </p>

      <p>
        The original paper describes a conservative
        <em>thickness heuristic</em> that is derived from the assumption that
        the thickness of an object is similar to its size in screen space; the
        end result of it is that âa single sample that is behind the horizon
        will not significantly decrease the computed horizon, but many of them
        (in e.g. a thin feature) will considerably attenuate itâ. In our
        experimentation we found that increasing the number of slices while
        undersampling the horizon search (using lower number of samples per
        slice) achieves very similar result with the same overall number of
        samples. This removes the need for the somewhat computationally
        expensive heuristic.
      </p>

      <p>
        We also experimented with a different heuristic that biases the
        near-field bounding falloff along the view vector, and in effect
        reducing the impact of samples that are in front of the evaluated
        pixelâs depth (closer to the camera plane). This provided results closer
        to the ground truth compared to the heuristic from the original paper
        and this is now exposed through the âThin occluder compensationâ
        setting. With 6 (3+3) samples per slice, the (auto-tuned) optimum
        setting value yields a relatively small improvement, so we disabled it
        by default for performance reasons. It can be easily enabled if higher
        quality is required.
      </p>

      <p>
        <img
          src="images/thin-occluders-heuristic.png"
          alt="thin occluders"
          width="100%"
        />
        <em
          >left: default âThin occluder compensationâ of 0; middle: ray traced
          ground truth; right: âThin occluder compensationâ of 0.7</em
        >
      </p>

      <p>
        Above image demonstrates two opposing scenarios: in the top row, even
        the default settings (left column) over-compensate the thin occluder
        issue due to shelves being very deep, and increasing
        <em>thin occluder compensation</em> setting (right column) serves only
        to further deviate from the ground truth (middle column). This is in
        contrast to the bottom row where pipe and chair legs are very thin, and
        a high <em>occluder compensation</em> setting (right column) matches
        ground truth more closely.
      </p>

      <h3 id="sampling-noise">Sampling noise</h3>

      <p>
        As with any technique based on Monte Carlo integration, a
        <a
          href="https://pbr-book.org/3ed-2018/Monte_Carlo_Integration/Careful_Sample_Placement"
          >a good sampling method can significantly reduce the number of samples
          needed for the same quality</a
        >. The original GTAO paper describes a tileable spatial noise of 4x4
        with 6 different temporal rotations.
      </p>

      <p>
        For stratified sampling we map screen coordinates to a Hilbert curve
        index, using it to drive Martin Robertâs
        <a
          href="http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/"
          >R2 quasi-random sequence</a
        >. This was inspired by an
        <a href="https://www.shadertoy.com/view/3tB3z3"
          >excellent shadertoy example by user âpaniqâ</a
        >
        with the only difference in our code being that we use a R2 sequence
        instead of R1 since we need two low-discrepancy samples for chosing
        slice angle and step offset. We use a 6 level Hilbert curve, providing a
        64x64 repeating tile, and for the temporal component we add an offset of
        <code>288*(frameIndex%64)</code>, found empirically.
      </p>

      <p>
        Before we settled on the Hilbert curve + R2 sequence, we used a 2
        channel 64x64 tileable blue noise from
        <a href="http://momentsingraphics.de/BlueNoise.html"
          >Christoph Petersâs blog</a
        >
        to drive slice rotations and individual sample noise offsets. This
        worked well for spatial-only noise but adding temporal offsets/rotations
        caused overlaps which would often show as temporal artifacts. We then
        switched to a 3D noise (from
        <a href="https://momentsingraphics.de/3DBlueNoise.html"
          >the sequel blogpost</a
        >
        which worked well with TAA but was fairly big in size and did not work
        well when using spatial-only filtering (to quote the blog, âGood 3D
        noise is a bad 2D blue noiseâ).
      </p>

      <p>
        Since computing Hilbert curve index in the compute shader adds
        measurable cost (~7%), we optionally precompute it into a lookup texture
        which reduces this overhead. C++/HLSL code to compute the Hilbert Index
        is available in XeGTAO.h and the user can choose between the (simpler)
        GPU arithmetic or (usually faster) LUT-based codepaths.
      </p>

      <p>
        <img
          src="images/random-vs-hilbert-r2.png"
          alt="reference raytracer"
        /><br />
        <em
          >5x5 spatial with 8 frame temporal filter, left: using hash-based
          pseudo-random noise; right: using Hilbert curve index driving R2
          sequence</em
        >
      </p>

      <h3 id="memory-bandwidth-bottleneck">Memory bandwidth bottleneck</h3>
      <p>
        Most screen space effects are performance-limited by the available
        memory bandwidth and texture caching efficiency, and XeGTAO is no
        different.
      </p>

      <p>
        One common approach, which we rely on, relies on a technique presented
        in <em>Scalable Ambient Obscurance</em>
        <a
          href="https://research.nvidia.com/sites/default/files/pubs/2012-06_Scalable-Ambient-Obscurance/McGuire12SAO.pdf"
          >[McGuire et al, 2012]</a
        >
        and involves pre-filtering depth buffer into MIP hierarchy, allowing the
        more distant (from the evaluated pixelâs center) locations to be sampled
        using lower detail MIP level. We follow the same approach as in the
        paper, with the exception of the choice of the depth MIP filter, for
        which we use a weighted average of samples, with the weight determined
        by whether the depth difference from the most distant sample is within a
        predefined threshold (please refer to DepthMIPFilter in the code for
        details). Using the most distant sample introduces a natural thin
        occluder bias and is more stable under motion compared to rotated grid
        subsampling (from the SAO paper), while averaging provides least
        precision errors on most slopes.
      </p>

      <p>
        <img src="images/depth-mips.png" alt="depth-mips" width="100%" />
        <em
          >left: color-coded sample MIP levels; middle: example of detail loss
          with a too low âDepth MIP sampling offsetâ of 2.0; right: depth MIP
          mapping disabled</em
        >
      </p>

      <p>
        âDepth MIP sampling offsetâ user setting controls the MIP level
        selection (<em
          >mipLevel = max( 0, log2( sampleOffsetLength ) -
          DepthMIPSamplingOffset )</em
        >). The lower the value, the lower detailed MIPs are used, reducing
        memory bandwidth but also reducing quality. It defaults to the value of
        3.15 which is the point below which there is no measurable performance
        increase on any of the tested hardware.
      </p>

      <p>
        Another popular solution to this problem is presented in
        <em
          >Deinterleaved Texturing for Cache-Efficient Interleaved Sampling</em
        >
        <a
          href="https://developer.nvidia.com/sites/default/files/akamai/gameworks/samples/DeinterleavedTexturing.pdf"
          >[Bavoil, 2014]</a
        >
        and involves a divide and conquer technique where the working dataset is
        subdivided into smaller parts that are processed in sequence, ensuring a
        much better utilization of memory cache structures. The downside is that
        by the definition, the processing of one dataset part can only rely on
        sampling data from that part, which constrains the sampling pattern.
        This was a significant issue for GTAO with its specific sampling pattern
        (samples lie on a straight line, etc.) where constraining them to a
        subset of data significantly limited flexibility, consistency and
        amplified precision issues.
      </p>

      <h3 id="misc">Misc</h3>
      <ul>
        <li>
          We added a global âFinal powerâ heuristic that modifies the visibility
          with a power function. While this has no basis in physical light
          transfer, we found that auto-tune can use it to achieve better ground
          truth match in combination with all other settings.
        </li>
        <li>
          In order to minimize bandwidth use we rely on 16-bit floating point
          buffer to store viewspace depth. This does cause some minor precision
          quality issues but yields better performance on most hardware. It is
          however not compatible with built-in screen space normal map
          generator.
        </li>
        <li>
          It is always advisable to provide screen space normals to the effect,
          but in case that is not possible we provide a built-in depth to normal
          map generator.
        </li>
        <li>
          We have enabled fp16 (half float) precision shader math on most places
          where the loss in precision was acceptable; this provides 5-20%
          performance boost on various hardware that we have tested on but is
          entirely optional.
        </li>
        <li>
          In the Bistro scene lighting we use the AO term to attenuate diffuse
          and specular irradiance from light probes using the multi-bounce
          diffuse and GTSO approaches detailed in the original GTAO work. We
          also slightly attenuate direct lighting using the micro-shadowing
          approximation from <em>Material Advances in Call of Duty: WWII</em>
          <a
            href="http://advances.realtimerendering.com/other/2016/naughty_dog/NaughtyDog_TechArt_Final.pdf"
            >[Chan 2018]</a
          >
          /
          <a
            href="http://advances.realtimerendering.com/other/2016/naughty_dog/index.html"
            >SIGGRAPH 2016: Technical Art of Uncharted</a
          >. Our current rendererâs AO term usage is ad-hoc and has not been
          matched to ground truth, and is not meant as a reference.
        </li>
      </ul>

      <p>
        <img
          src="images/interior-assao-medium-vs-gtao-high.png"
          alt="ASSAO vs GTAO"
          width="100%"
        />
        <em
          >left: ASSAO Medium (~0.72ms*), right: XeGTAO High (~0.56ms*) (*as
          measured on RTX 2060 at 1920x1080)</em
        >
      </p>

      <h3 id="faq">FAQ</h3>

      <ul>
        <li>
          <strong>Q:</strong> It is still too slow for our target platform, what
          are our options?
        </li>
        <li>
          <strong>A:</strong> The âMediumâ quality preset is roughly 2/3 of the
          cost of the âHighâ preset (for ex., 1.5ms vs 2.2ms at 1920x1080, GTX
          1050), while the âLowâ quality preset is roughly 2/3 of the cost of
          the âMediumâ preset. For anything faster we advise further reducing
          sliceCount (in the call XeGTAO_MainPass) at the expense of more noise,
          or using lower resolution rendering (half by half or checkerboard) and
          upgrading the denoiser pass with a bilateral upsample. ___
        </li>
        <li>
          <strong>Q:</strong> Why is there support for both half (fp16) and
          single (fp32) precision shader paths?
        </li>
        <li>
          <strong>A:</strong> While the quality loss on the fp16 path is
          minimal, we found that some GPUs can suffer from unexpected
          performance regression on it, sometimes depending on the driver
          version. For that reason, while enabled by default, we leave it as an
          optional switch. ___
        </li>
        <li><strong>Q:</strong> Any plans for a Vulkan port?</li>
        <li>
          <strong>A:</strong> Upgrades to other platforms/APIs will be added
          based on interest. Please feel free to submit an issue with a request.
          ___
        </li>
      </ul>

      <p><br /></p>

      <h2 id="version-log">Version log</h2>

      <p>
        See
        <a
          href="https://github.com/GameTechDev/XeGTAO/tree/master/Source/Rendering/Shaders/XeGTAO.h#L14"
          >XeGTAO.h</a
        >
      </p>

      <h2 id="authors">Authors</h2>

      <p>
        XeGTAO was created by Filip Strugar and Steve Mccalla, feel free to send
        any feedback directly to filip.strugar@intel.com and
        stephen.mccalla@intel.com.
      </p>

      <h2 id="credits">Credits</h2>

      <p>
        Many thanks to Jorge Jimenez, Xian-Chun Wu, Angelo Pesce and Adrian
        Jarabo, authors of the original paper. This implementation would not be
        possible without their seminal work.
      </p>

      <p>
        Thanks to Trapper McFerron for implementing the DoF effect and other
        things, Lukasz Migas for his excellent
        <a href="https://github.com/GameTechDev/TAA">TAA implementation</a>,
        Andrew Helmer (https://andrewhelmer.com/) for help with the
        Owen-Scrambled Sobol noise sequences, Adam Lake and David Bookout for
        reviews, bug reports and valuable suggestions!
      </p>

      <p>
        Many thanks to: Amazon and Nvidia for providing the Amazon Lumberyard
        Bistro dataset through the Open Research Content Archive (ORCA):
        https://developer.nvidia.com/orca/amazon-lumberyard-bistro; author of
        the
        <a
          href="https://sketchfab.com/3d-models/sf-light-fighter-x6-24a995860c51424da2d93e23d0c0ec57"
          >spaceship model available on Sketchfab</a
        >; Khronos Group for providing the Flight Helmet model and other
        <a
          href="https://github.com/KhronosGroup/glTF-Sample-Models/tree/master/2.0"
          >reference GLTF models</a
        >.
      </p>

      <p>
        Many thanks to the developers of the following open-source libraries or
        projects that make the Vanilla sample framework possible:
      </p>
      <ul>
        <li>dear imgui (https://github.com/ocornut/imgui)</li>
        <li>assimp (https://github.com/assimp/assimp)</li>
        <li>DirectXTex (https://github.com/Microsoft/DirectXTex)</li>
        <li>
          DirectX Shader Compiler
          (https://github.com/microsoft/DirectXShaderCompiler)
        </li>
        <li>Filament (https://github.com/google/filament)</li>
        <li>
          Game Task Scheduler
          (https://github.com/GameTechDev/GTS-GamesTaskScheduler)
        </li>
        <li>Cpp-Taskflow (https://github.com/cpp-taskflow/cpp-taskflow)</li>
        <li>tinyxml2 (https://github.com/leethomason/tinyxml2)</li>
        <li>zlib (https://zlib.net/)</li>
        <li>meshoptimizer (https://github.com/zeux/meshoptimizer)</li>
        <li>nlohmann/json (https://github.com/nlohmann/json)</li>
        <li>
          Christoph Petersâ blue noise
          (http://momentsingraphics.de/BlueNoise.html)
        </li>
        <li>Lukasz Migasâs TAA (https://github.com/GameTechDev/TAA)</li>
        <li>â¦and any I might have forgotten (please let me know) :)</li>
      </ul>

      <h2 id="references">References</h2>

      <ul>
        <li>
          <strong>[Jimenez et al., 2016]</strong>: GTAO, Practical Realtime
          Strategies for Accurate Indirect Occlusion, Jimenez et al., 2016,
          <a
            href="https://www.activision.com/cdn/research/Practical_Real_Time_Strategies_for_Accurate_Indirect_Occlusion_NEW%20VERSION_COLOR.pdf"
            >https://www.activision.com/cdn/research/Practical_Real_Time_Strategies_for_Accurate_Indirect_Occlusion_NEW%20VERSION_COLOR.pdf</a
          >
        </li>
        <li>
          <strong>[HBAO+]</strong>: Horizon Based Ambient Occlusion Plus,
          https://www.nvidia.com/en-gb/geforce/technologies/hbao-plus/technology/
        </li>
        <li>
          <strong>[ASSAO]</strong>: Adaptive Screen Space Ambient Occlusion,
          https://software.intel.com/content/www/us/en/develop/articles/adaptive-screen-space-ambient-occlusion.html
        </li>
        <li>
          <strong>[Vardis et al. 2013]</strong>: Multi-view ambient occlusion
          with importance sampling, Kostas Vardis, Georgios Papaioannou,
          Athanasios Gaitatzes, https://dl.acm.org/doi/10.1145/2448196.2448214
        </li>
        <li>
          <strong>[Vermeer et al, 2021]</strong>: Stochastic-Depth Ambient
          Occlusion, Jop Vermeer, Leonardo Scandolo, Elmar Eisemann,
          https://dl.acm.org/doi/10.1145/3451268
        </li>
        <li>
          <strong>Careful sample placement</strong>: Physically Based
          Rendering:From Theory To Implementation, Matt Pharr, Wenzel Jakob, and
          Greg Humphreys, Careful sample placement 13.8,
          https://pbr-book.org/3ed-2018/Monte_Carlo_Integration/Careful_Sample_Placement
        </li>
        <li>
          <strong
            >The Unreasonable Effectiveness of Quasirandom Sequences</strong
          >: Martin Roberts 2018,
          http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/
        </li>
        <li>
          <strong>Hilbert R1 Blue Noise</strong>: paniq,
          https://www.shadertoy.com/view/3tB3z3
        </li>
        <li>
          <strong>Free blue noise textures</strong>: Moments in Graphics, A blog
          by Christoph Peters, http://momentsingraphics.de/BlueNoise.html
        </li>
        <li>
          <strong>The problem with 3D blue noise</strong>: Moments in Graphics,
          A blog by Christoph Peters,
          http://momentsingraphics.de/3DBlueNoise.html
        </li>
        <li>
          <strong>[McGuire et al, 2012]</strong>: Scalable Ambient Obscurance,
          Morgan McGuire. Michael Mara, David Luebke,
          https://research.nvidia.com/sites/default/files/pubs/2012-06_Scalable-Ambient-Obscurance/McGuire12SAO.pdf
        </li>
        <li>
          <strong>[Bavoil, 2014]</strong>, Deinterleaved Texturing for
          Cache-Efficient Interleaved Sampling, Louis Bavoil,
          https://developer.nvidia.com/sites/default/files/akamai/gameworks/samples/DeinterleavedTexturing.pdf
        </li>
        <li>
          <strong>[Chan 2018]</strong>, Material Advances in Call of Duty: WWII_
          (https://www.activision.com/cdn/research/MaterialAdvancesInWWII.pdf)
        </li>
        <li>
          <strong>[SIGGRAPH 2016: Technical Art of Uncharted]</strong>,
          http://advances.realtimerendering.com/other/2016/naughty_dog/index.html
        </li>
      </ul>

      <h2 id="license">License</h2>

      <p>
        Sample and its code provided under MIT license, please see
        <a href="/LICENSE">LICENSE</a>. All third-party source code provided
        under their own respective and MIT-compatible Open Source licenses.
      </p>

      <p>Copyright (C) 2021, Intel Corporation</p>
    </main>
    <footer>
      <!--IGHF Footer-->
      <!--GAATversion='50recode.2' date='09/11/2017 08:00:00' Version='2.0':CharacterEncoding:utf8-->
      <div id="recode50footer"></div>
      <script type="text/javascript">
        /*<![CDATA[*/
        INTELNAV = window.INTELNAV || {};
        INTELNAV.renderSettingsFooter = {
          version: "2.0 - 03/12/2017 08:00:00",
          OutputId: "gf_default",
        };
        /*]]>*/
      </script>
      <noscript>
        <div id="smallfootprint-footer">
          <ul>
            <li>Â©Intel Corporation</li>
            <li>
              <a
                href="https://www.intel.com/content/www/us/en/legal/terms-of-use.html"
                target=""
                >Terms of Use</a
              >
            </li>
            <li>
              <a
                href="https://www.intel.com/content/www/us/en/legal/trademarks.html"
                target=""
                >*Trademarks</a
              >
            </li>
            <li>
              <a
                href="https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html"
                target=""
                >Privacy</a
              >
            </li>
            <li>
              <a
                href="https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html"
                target=""
                >Cookies</a
              >
            </li>
            <li>
              <a
                href="https://www.intel.com/content/www/us/en/policy/policy-human-trafficking-and-slavery.html"
                target=""
                >Supply Chain Transparency
              </a>
            </li>
            <li>
              <a
                href="https://www.intel.com/content/www/us/en/siteindex.html"
                target=""
                >Site Map</a
              >
            </li>
          </ul>
        </div>
      </noscript>
      <!--/IGHF Footer-->
    </footer>

    <script type="text/javascript">
      //configure tms
      var wapLocalCode = "us-en"; //dynamically set per localized site, see mapping table for values
      var wapSection = "gametechdev"; //section, specific for each site
      //load tms
      (function () {
        var host =
          window.document.location.protocol == "http:"
            ? "http://www.intel.com"
            : "https://www.intel.com";
        var url = host + "/content/dam/www/global/wap/tms-loader.js"; //wap file url
        var po = document.createElement("script");
        po.type = "text/javascript";
        po.async = true;
        po.src = url;
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(po, s);
      })();
    </script>
  </body>
</html>
